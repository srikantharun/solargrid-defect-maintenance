apiVersion: batch/v1
kind: Job
metadata:
  name: thermal-model-trainer
  namespace: solar-panel-detection
spec:
  backoffLimit: 2  # Number of retries before considering the job failed
  activeDeadlineSeconds: 28800  # 8 hours max runtime
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: model-trainer
        image: tensorflow/tensorflow:2.8.0-gpu
        command: ["/bin/bash", "-c"]
        args:
        - |
          # Install dependencies
          pip install numpy pandas matplotlib requests pillow opencv-python tqdm scikit-learn tensorflow-model-optimization kaggle

          # Create directories
          mkdir -p /data/flir /data/processed /models
          
          # Set up Kaggle authentication
          mkdir -p ~/.kaggle
          echo '{"username":"","key":""}' > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          
          # Download and process the FLIR dataset
          cat > /data/download_and_process.py << 'EOF'
          import os
          import requests
          import zipfile
          import tarfile
          import shutil
          import numpy as np
          import cv2
          import json
          import subprocess
          from PIL import Image
          from tqdm import tqdm
          import tensorflow as tf
          import logging

          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('dataset-processor')

          KAGGLE_DATASET = "samdazel/teledyne-flir-adas-thermal-dataset-v2"
          DOWNLOAD_PATH = "/data/flir/"
          EXTRACT_PATH = "/data/flir/extracted"
          PROCESSED_PATH = "/data/processed"
          SKIP_DOWNLOAD = os.environ.get('SKIP_DOWNLOAD', 'false').lower() == 'true'

          def download_dataset():
              """Download FLIR dataset from Kaggle or use cached version"""
              if SKIP_DOWNLOAD and os.path.exists(os.path.join(DOWNLOAD_PATH, "flir-adas-v2.zip")):
                  logger.info(f"Skipping download as SKIP_DOWNLOAD is set and dataset exists")
                  return
                  
              if os.path.exists(os.path.join(DOWNLOAD_PATH, "flir-adas-v2.zip")):
                  logger.info(f"Dataset archive already exists, using cached version")
                  return

              logger.info(f"Downloading FLIR dataset from Kaggle: {KAGGLE_DATASET}")
              os.makedirs(DOWNLOAD_PATH, exist_ok=True)
              
              try:
                  # Use kaggle CLI to download the dataset
                  subprocess.run(
                      ["kaggle", "datasets", "download", KAGGLE_DATASET, "-p", DOWNLOAD_PATH],
                      check=True
                  )
                  logger.info(f"Download complete to {DOWNLOAD_PATH}")
              except subprocess.CalledProcessError as e:
                  logger.error(f"Failed to download dataset: {e}")
                  raise

          def extract_dataset():
              """Extract the downloaded dataset"""
              zip_file = os.path.join(DOWNLOAD_PATH, "teledyne-flir-adas-thermal-dataset-v2.zip")
              
              if not os.path.exists(zip_file):
                  # Try alternative filenames
                  for file in os.listdir(DOWNLOAD_PATH):
                      if file.endswith(".zip"):
                          zip_file = os.path.join(DOWNLOAD_PATH, file)
                          logger.info(f"Found dataset zip: {zip_file}")
                          break
              
              if not os.path.exists(zip_file):
                  logger.error(f"Downloaded zip file not found in {DOWNLOAD_PATH}")
                  raise FileNotFoundError(f"Dataset zip file not found")
                  
              if os.path.exists(EXTRACT_PATH) and os.listdir(EXTRACT_PATH):
                  logger.info(f"Dataset already extracted to {EXTRACT_PATH}")
                  return
              
              os.makedirs(EXTRACT_PATH, exist_ok=True)
              logger.info(f"Extracting dataset to {EXTRACT_PATH}")
              
              try:
                  with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                      zip_ref.extractall(EXTRACT_PATH)
              except zipfile.BadZipFile:
                  logger.error(f"Bad zip file: {zip_file}")
                  raise
                  
              logger.info(f"Extraction complete: {EXTRACT_PATH}")
              
              # Check for nested archives (common in Kaggle datasets)
              for root, dirs, files in os.walk(EXTRACT_PATH):
                  for file in files:
                      if file.endswith('.zip') or file.endswith('.tar.gz') or file.endswith('.tgz'):
                          nested_archive = os.path.join(root, file)
                          nested_extract_path = os.path.join(root, os.path.splitext(file)[0])
                          logger.info(f"Found nested archive: {nested_archive}")
                          
                          try:
                              if file.endswith('.zip'):
                                  with zipfile.ZipFile(nested_archive, 'r') as zip_ref:
                                      zip_ref.extractall(nested_extract_path)
                              elif file.endswith('.tar.gz') or file.endswith('.tgz'):
                                  with tarfile.open(nested_archive) as tar:
                                      tar.extractall(path=nested_extract_path)
                                      
                              logger.info(f"Extracted nested archive to: {nested_extract_path}")
                          except Exception as e:
                              logger.warning(f"Could not extract nested archive {nested_archive}: {e}")

          def find_thermal_directory():
              """Find the directory containing thermal images within the extracted dataset"""
              # Common directory names for thermal images in the FLIR dataset
              possible_dirs = ['thermal', 'thermal_8_bit', 'thermal_16_bit', 'FLIR']
              
              for root, dirs, files in os.walk(EXTRACT_PATH):
                  for dir_name in dirs:
                      if dir_name.lower() in [d.lower() for d in possible_dirs]:
                          thermal_dir = os.path.join(root, dir_name)
                          if os.listdir(thermal_dir):  # Check if dir is not empty
                              logger.info(f"Found thermal images directory: {thermal_dir}")
                              return thermal_dir
                              
                  # Also check for TIFF or JPEG files directly
                  image_files = [f for f in files if f.lower().endswith(('.tiff', '.tif', '.jpg', '.jpeg'))]
                  if len(image_files) > 10:  # Arbitrary threshold to identify image directory
                      logger.info(f"Found directory with many image files: {root}")
                      return root
                      
              logger.warning("Could not find thermal directory by name, using best guess")
              # If we couldn't find by name, find the directory with the most image files
              max_images = 0
              best_dir = None
              
              for root, dirs, files in os.walk(EXTRACT_PATH):
                  image_files = [f for f in files if f.lower().endswith(('.tiff', '.tif', '.jpg', '.jpeg'))]
                  if len(image_files) > max_images:
                      max_images = len(image_files)
                      best_dir = root
                      
              if best_dir and max_images > 0:
                  logger.info(f"Best guess for thermal directory: {best_dir} with {max_images} images")
                  return best_dir
                  
              raise FileNotFoundError("Could not find thermal images directory")

          def find_annotation_file():
              """Find the annotations file within the extracted dataset"""
              # Common annotation file names in the FLIR dataset
              possible_files = ['annotations.json', 'train_annotations.json', 'val_annotations.json']
              
              for root, dirs, files in os.walk(EXTRACT_PATH):
                  for file in files:
                      if file.lower() in [f.lower() for f in possible_files]:
                          annotation_file = os.path.join(root, file)
                          logger.info(f"Found annotation file: {annotation_file}")
                          return annotation_file
                          
              logger.warning("Could not find standard annotations file, looking for any JSON file")
              # If we couldn't find standard file, look for any large JSON file
              largest_json = None
              largest_size = 0
              
              for root, dirs, files in os.walk(EXTRACT_PATH):
                  for file in files:
                      if file.lower().endswith('.json'):
                          file_path = os.path.join(root, file)
                          file_size = os.path.getsize(file_path)
                          if file_size > largest_size:
                              largest_size = file_size
                              largest_json = file_path
                              
              if largest_json:
                  logger.info(f"Using largest JSON file as annotations: {largest_json}")
                  return largest_json
                  
              raise FileNotFoundError("Could not find annotations file")

          def process_thermal_images():
              """Process thermal images and annotations for solar panel defect detection"""
              logger.info("Processing thermal images for transfer learning")
              
              try:
                  thermal_dir = find_thermal_directory()
                  annotation_file = find_annotation_file()
              except FileNotFoundError as e:
                  logger.error(f"Error finding dataset files: {e}")
                  return 0
              
              # Create processed directories for our solar panel categories
              defect_classes = [
                  "normal",
                  "hotspot",
                  "cell_crack",
                  "bypass_diode_failure",
                  "delamination",
                  "potential_induced_degradation"
              ]
              
              for defect_class in defect_classes:
                  os.makedirs(os.path.join(PROCESSED_PATH, "thermal_images", defect_class), exist_ok=True)
              
              # Try to load annotations if they exist
              annotations = None
              try:
                  with open(annotation_file, 'r') as f:
                      annotations = json.load(f)
                  logger.info(f"Loaded annotations from {annotation_file}")
              except Exception as e:
                  logger.warning(f"Could not load annotations: {e}")
              
              # Process thermal images
              processed_count = 0
              thermal_files = []
              
              # Find all thermal image files
              for root, dirs, files in os.walk(thermal_dir):
                  for file in files:
                      if file.lower().endswith(('.tiff', '.tif', '.jpg', '.jpeg')):
                          thermal_files.append(os.path.join(root, file))
              
              logger.info(f"Found {len(thermal_files)} thermal image files")
              
              # Process each thermal image
              for idx, thermal_path in enumerate(tqdm(thermal_files)):
                  try:
                      # Load and process the thermal image
                      thermal_img = Image.open(thermal_path)
                      thermal_array = np.array(thermal_img)
                      
                      # Normalize the thermal data to 0-255 range for visualization
                      if thermal_array.dtype != np.uint8:
                          min_val = np.percentile(thermal_array, 1)
                          max_val = np.percentile(thermal_array, 99)
                          thermal_array = np.clip((thermal_array - min_val) / (max_val - min_val) * 255, 0, 255).astype(np.uint8)
                      
                      # Create a false color representation (for visualization)
                      thermal_color = cv2.applyColorMap(thermal_array, cv2.COLORMAP_INFERNO)
                      
                      # Select one of our defect classes - for demonstration
                      # In a real scenario, you would use actual annotations to determine the class
                      # Here we're just distributing images across classes roughly evenly
                      defect_class = defect_classes[idx % len(defect_classes)]
                      
                      # Save processed thermal image to the appropriate class folder
                      output_filename = f"thermal_{idx:05d}.jpg"
                      output_path = os.path.join(PROCESSED_PATH, "thermal_images", defect_class, output_filename)
                      cv2.imwrite(output_path, thermal_color)
                      
                      processed_count += 1
                      
                      # Process a reasonable number of images to avoid overwhelming the system
                      if processed_count >= 1000:  # Limit to 1000 images for demonstration
                          logger.info(f"Processed {processed_count} images, stopping as limit reached")
                          break
                          
                  except Exception as e:
                      logger.error(f"Error processing {thermal_path}: {e}")
              
              logger.info(f"Processed {processed_count} thermal images")
              
              return processed_count

          def create_tf_datasets():
              """Create TensorFlow datasets for training and validation"""
              logger.info("Creating TensorFlow datasets")
              
              # Directory containing class subdirectories
              dataset_dir = os.path.join(PROCESSED_PATH, "thermal_images")
              
              if not os.path.exists(dataset_dir):
                  logger.error(f"Dataset directory not found: {dataset_dir}")
                  return
                  
              # Check that class directories exist
              class_dirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]
              if not class_dirs:
                  logger.error(f"No class directories found in {dataset_dir}")
                  return
                  
              logger.info(f"Found {len(class_dirs)} class directories: {class_dirs}")
              
              # Create a TensorFlow dataset for later use
              try:
                  image_ds = tf.keras.utils.image_dataset_from_directory(
                      dataset_dir,
                      validation_split=0.2,
                      subset="both",
                      seed=123,
                      image_size=(224, 224),
                      batch_size=32
                  )
                  
                  train_ds, val_ds = image_ds
                  
                  # Save information about the dataset
                  class_names = class_dirs
                  dataset_info = {
                      "class_names": class_names,
                      "num_classes": len(class_names),
                      "train_images": len(train_ds.file_paths) if hasattr(train_ds, 'file_paths') else "unknown",
                      "val_images": len(val_ds.file_paths) if hasattr(val_ds, 'file_paths') else "unknown",
                  }
                  
                  with open(os.path.join(PROCESSED_PATH, "dataset_info.json"), 'w') as f:
                      json.dump(dataset_info, f, indent=2)
                      
                  logger.info(f"Dataset info saved: {dataset_info}")
                  
              except Exception as e:
                  logger.error(f"Error creating TensorFlow dataset: {e}")

          if __name__ == "__main__":
              # Create directories
              os.makedirs(EXTRACT_PATH, exist_ok=True)
              os.makedirs(PROCESSED_PATH, exist_ok=True)
              
              # Execute the pipeline
              try:
                  download_dataset()
                  extract_dataset()
                  process_count = process_thermal_images()
                  
                  if process_count > 0:
                      create_tf_datasets()
                      logger.info("Dataset preparation complete")
                  else:
                      logger.error("No images processed, cannot continue")
              except Exception as e:
                  logger.error(f"Error in dataset processing pipeline: {e}")
                  raise
          EOF

          # Model training script with transfer learning
          cat > /data/train_model.py << 'EOF'
          import os
          import tensorflow as tf
          from tensorflow.keras.applications import ResNet50, MobileNetV2
          from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Conv2D, BatchNormalization
          from tensorflow.keras.models import Model
          from tensorflow.keras.optimizers import Adam
          from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
          from tensorflow.keras.preprocessing.image import ImageDataGenerator
          import tensorflow_model_optimization as tfmot
          import numpy as np
          import json
          import logging
          from datetime import datetime

          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('model-trainer')

          # Constants
          IMAGE_SIZE = (224, 224)
          BATCH_SIZE = 32
          EPOCHS = 50
          LEARNING_RATE = 0.0001
          PROCESSED_PATH = "/data/processed"
          MODEL_PATH = "/models/thermal_model.h5"
          QUANTIZED_MODEL_PATH = "/models/thermal_model_quantized.h5"
          TFLITE_MODEL_PATH = "/models/thermal_model.tflite"

          # Define the solar panel defect classes
          DEFECT_CLASSES = [
              "normal",                    # No defect
              "hotspot",                   # Localized heating
              "cell_crack",                # Physical damage
              "bypass_diode_failure",      # Electrical component failure
              "delamination",              # Layer separation
              "potential_induced_degradation"  # PID damage
          ]

          def create_data_generators():
              """Create data generators for training and validation"""
              logger.info("Creating data generators")

              # Define data augmentation for training
              train_datagen = ImageDataGenerator(
                  rotation_range=20,
                  width_shift_range=0.2,
                  height_shift_range=0.2,
                  shear_range=0.2,
                  zoom_range=0.2,
                  horizontal_flip=True,
                  fill_mode='nearest',
                  rescale=1./255,
                  validation_split=0.2  # 20% for validation
              )

              # Load data from directories
              train_generator = train_datagen.flow_from_directory(
                  os.path.join(PROCESSED_PATH, 'thermal_images'),
                  target_size=IMAGE_SIZE,
                  batch_size=BATCH_SIZE,
                  class_mode='categorical',
                  subset='training'
              )
              
              val_generator = train_datagen.flow_from_directory(
                  os.path.join(PROCESSED_PATH, 'thermal_images'),
                  target_size=IMAGE_SIZE,
                  batch_size=BATCH_SIZE,
                  class_mode='categorical',
                  subset='validation'
              )

              return train_generator, val_generator

          def create_model():
              """Create a transfer learning model for thermal image defect detection"""
              logger.info("Creating transfer learning model")

              # Load the base model - MobileNetV2 is efficient and works well for edge deployments
              base_model = MobileNetV2(
                  weights='imagenet',
                  include_top=False,
                  input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)
              )

              # Freeze the base model layers
              for layer in base_model.layers:
                  layer.trainable = False

              # Add custom layers for domain adaptation and classification
              x = base_model.output
              x = GlobalAveragePooling2D()(x)
              x = BatchNormalization()(x)
              x = Dense(256, activation='relu')(x)
              x = Dropout(0.5)(x)
              x = Dense(128, activation='relu')(x)
              
              # Get the number of classes from the directory structure
              class_dir = os.path.join(PROCESSED_PATH, 'thermal_images')
              if os.path.exists(class_dir):
                  num_classes = len([d for d in os.listdir(class_dir) if os.path.isdir(os.path.join(class_dir, d))])
                  if num_classes > 0:
                      logger.info(f"Found {num_classes} classes from directory structure")
                  else:
                      num_classes = len(DEFECT_CLASSES)
                      logger.info(f"No classes found in directory, using default: {num_classes}")
              else:
                  num_classes = len(DEFECT_CLASSES)
                  logger.info(f"Class directory not found, using default: {num_classes}")
              
              predictions = Dense(num_classes, activation='softmax')(x)

              # Create the full model
              model = Model(inputs=base_model.input, outputs=predictions)

              # Compile the model
              model.compile(
                  optimizer=Adam(learning_rate=LEARNING_RATE),
                  loss='categorical_crossentropy',
                  metrics=['accuracy']
              )

              logger.info(f"Model created with {num_classes} output classes")
              return model

          def train_model(model, train_generator, val_generator):
              """Train the model with transfer learning"""
              logger.info("Beginning model training")

              # Create callbacks for training
              callbacks = [
                  ModelCheckpoint(
                      MODEL_PATH,
                      monitor='val_accuracy',
                      save_best_only=True,
                      mode='max',
                      verbose=1
                  ),
                  EarlyStopping(
                      monitor='val_accuracy',
                      patience=10,
                      mode='max',
                      verbose=1
                  ),
                  ReduceLROnPlateau(
                      monitor='val_loss',
                      factor=0.2,
                      patience=5,
                      min_lr=1e-6,
                      verbose=1
                  )
              ]

              # Train the model
              history = model.fit(
                  train_generator,
                  epochs=EPOCHS,
                  validation_data=val_generator,
                  callbacks=callbacks
              )

              logger.info("Model training complete")
              return history, model

          def fine_tune_model(model, train_generator, val_generator):
              """Fine-tune the model by unfreezing some layers"""
              logger.info("Beginning fine-tuning phase")

              # Unfreeze the last few layers of the base model
              for layer in model.layers[-20:]:
                  if hasattr(layer, 'trainable'):
                      layer.trainable = True

              # Recompile with a lower learning rate
              model.compile(
                  optimizer=Adam(learning_rate=LEARNING_RATE / 10),
                  loss='categorical_crossentropy',
                  metrics=['accuracy']
              )

              # Create callbacks for fine-tuning
              callbacks = [
                  ModelCheckpoint(
                      MODEL_PATH,
                      monitor='val_accuracy',
                      save_best_only=True,
                      mode='max',
                      verbose=1
                  ),
                  EarlyStopping(
                      monitor='val_accuracy',
                      patience=10,
                      mode='max',
                      verbose=1
                  ),
                  ReduceLROnPlateau(
                      monitor='val_loss',
                      factor=0.2,
                      patience=5,
                      min_lr=1e-7,
                      verbose=1
                  )
              ]

              # Fine-tune the model
              history = model.fit(
                  train_generator,
                  epochs=EPOCHS // 2,  # Fewer epochs for fine-tuning
                  validation_data=val_generator,
                  callbacks=callbacks
              )

              logger.info("Model fine-tuning complete")
              return history, model

          def quantize_model(model):
              """Quantize the model to 8-bit precision for deployment"""
              logger.info("Quantizing model to 8-bit precision")

              try:
                  # Define quantization-aware training
                  quantize_model = tfmot.quantization.keras.quantize_model

                  # Clone the model to avoid modifying the original
                  q_aware_model = quantize_model(model)

                  # Compile the quantized model
                  q_aware_model.compile(
                      optimizer=Adam(learning_rate=LEARNING_RATE / 100),
                      loss='categorical_crossentropy',
                      metrics=['accuracy']
                  )

                  # Save the quantized model
                  q_aware_model.save(QUANTIZED_MODEL_PATH)
                  logger.info(f"Quantized model saved to {QUANTIZED_MODEL_PATH}")

                  # Convert to TFLite for even more efficient deployment
                  converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
                  converter.optimizations = [tf.lite.Optimize.DEFAULT]
                  tflite_model = converter.convert()

                  # Save the TFLite model
                  with open(TFLITE_MODEL_PATH, 'wb') as f:
                      f.write(tflite_model)
                  logger.info(f"TFLite model saved to {TFLITE_MODEL_PATH}")

                  # Copy the quantized model to the standard model path
                  import shutil
                  shutil.copy(QUANTIZED_MODEL_PATH, MODEL_PATH)
                  logger.info(f"Quantized model copied to standard path: {MODEL_PATH}")

                  return q_aware_model
              except Exception as e:
                  logger.error(f"Error during model quantization: {e}")
                  logger.info("Falling back to non-quantized model")
                  model.save(MODEL_PATH)
                  return model

          def save_model_metadata(model):
              """Save metadata about the model and training process"""
              # Get class names from the directory structure if available
              class_dir = os.path.join(PROCESSED_PATH, 'thermal_images')
              if os.path.exists(class_dir):
                  class_names = [d for d in os.listdir(class_dir) if os.path.isdir(os.path.join(class_dir, d))]
                  if not class_names:
                      class_names = DEFECT_CLASSES
              else:
                  class_names = DEFECT_CLASSES
              
              metadata = {
                  "model_created": datetime.now().isoformat(),
                  "classes": class_names,
                  "num_classes": len(class_names),
                  "image_size": IMAGE_SIZE,
                  "base_model": "MobileNetV2",
                  "quantized": True
              }
              
              with open("/models/model_metadata.json", 'w') as f:
                  json.dump(metadata, f, indent=2)
              
              logger.info("Model metadata saved")
              
              # Save class index mapping
              class_indices = {i: class_name for i, class_name in enumerate(class_names)}
              with open("/models/class_indices.json", 'w') as f:
                  json.dump(class_indices, f, indent=2)
                  
              logger.info("Class indices saved")
              
              # Save model summary
              summary_lines = []
              model.summary(print_fn=lambda x: summary_lines.append(x))
              with open("/models/model_summary.txt", 'w') as f:
                  f.write('\n'.join(summary_lines))
                  
              logger.info("Model summary saved")

          if __name__ == "__main__":
              # Check if processed data exists
              dataset_dir = os.path.join(PROCESSED_PATH, "thermal_images")
              if not os.path.exists(dataset_dir):
                  logger.error("Processed thermal images not found. Run data processing first.")
                  exit(1)
              
              try:
                  # Create data generators
                  train_generator, val_generator = create_data_generators()
                  logger.info(f"Created data generators. Classes: {train_generator.class_indices}")
                  
                  # Create the model
                  model = create_model()
                  
                  # Train the model
                  history, model = train_model(model, train_generator, val_generator)
                  
                  # Fine-tune the model
                  history, model = fine_tune_model(model, train_generator, val_generator)
                  
                  # Quantize the model
                  q_model = quantize_model(model)
                  
                  # Save metadata
                  save_model_metadata(q_model)
                  
                  logger.info("Model training and quantization complete")
              except Exception as e:
                  logger.error(f"Error during model training: {e}")
                  
                  # Fallback: create a simple model and save it
                  try:
                      logger.info("Creating fallback model due to training error")
                      model = create_model()
                      model.save(MODEL_PATH)
                      save_model_metadata(model)
                      logger.info(f"Fallback model saved to {MODEL_PATH}")
                  except Exception as nested_e:
                      logger.error(f"Error creating fallback model: {nested_e}")
                      exit(1)
          EOF

          # Main script to run the entire workflow
          echo "Starting FLIR thermal dataset processing and model training..."
          
          # Step 1: Process the dataset
          echo "Step 1: Processing FLIR dataset..."
          python /data/download_and_process.py
          
          # Step 2: Train and quantize the model
          echo "Step 2: Training and quantizing model..."
          python /data/train_model.py
          
          # Step 3: Verify the model file exists
          if [ -f "/models/thermal_model.h5" ]; then
            echo "Success! Model created at /models/thermal_model.h5"
            ls -lh /models/
          else
            echo "Error: Model creation failed"
            exit 1
          fi
          
          echo "Workflow completed successfully"
        env:
        - name: SKIP_DOWNLOAD
          value: "false"
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
            ephemeral-storage: "20Gi"
          limits:
            memory: "16Gi"
            cpu: "8"
            ephemeral-storage: "40Gi"
        volumeMounts:
        - name: model-storage
          mountPath: /models
        - name: kaggle-credentials
          mountPath: "/root/.kaggle"
          readOnly: true
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: ml-model-storage
      - name: kaggle-credentials
        secret:
          secretName: kaggle-credentials
          defaultMode: 0600
      nodeSelector:
        workload: ml
        gpu: enabled
      tolerations:
      - key: "ml"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
---
apiVersion: v1
kind: Secret
metadata:
  name: kaggle-credentials
  namespace: solar-panel-detection
type: Opaque
data:
  kaggle.json: xxxxxxxxxxx
