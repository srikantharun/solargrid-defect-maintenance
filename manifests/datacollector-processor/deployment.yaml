apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-collector
  namespace: solar-panel-detection
spec:
  replicas: 1
  selector:
    matchLabels:
      app: data-collector
  template:
    metadata:
      labels:
        app: data-collector
    spec:
      containers:
      - name: data-collector
        image: python:3.9-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install flask requests redis pymongo numpy kafka-python && 
          mkdir -p /app && 
          cat > /app/collector.py << 'EOF'
          import os
          import json
          import time
          import logging
          import threading
          from datetime import datetime, timedelta
          from flask import Flask, request, jsonify
          import redis
          import pymongo
          from pymongo import MongoClient
          from bson import ObjectId
          import numpy as np

          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('data-collector')

          # Configuration from environment variables
          REDIS_HOST = os.environ.get('REDIS_HOST', 'redis')
          REDIS_PORT = int(os.environ.get('REDIS_PORT', '6379'))
          MONGO_URI = os.environ.get('MONGO_URI', 'mongodb://mongodb:27017/')
          MONGO_DB = os.environ.get('MONGO_DB', 'solar_panel_data')
          KAFKA_BROKER = os.environ.get('KAFKA_BROKER', 'kafka:9092')
          USE_KAFKA = os.environ.get('USE_KAFKA', 'false').lower() == 'true'
          KAFKA_TOPIC = os.environ.get('KAFKA_TOPIC', 'sensor-data')
          DATA_RETENTION_DAYS = int(os.environ.get('DATA_RETENTION_DAYS', '30'))
          API_PORT = int(os.environ.get('API_PORT', '8080'))

          # Flask application
          app = Flask(__name__)

          # Global connections
          redis_client = None
          mongo_client = None
          mongodb = None
          kafka_producer = None

          # Custom JSON encoder to handle MongoDB ObjectId
          class MongoJSONEncoder(json.JSONEncoder):
              def default(self, obj):
                  if isinstance(obj, ObjectId):
                      return str(obj)
                  return super(MongoJSONEncoder, self).default(obj)

          # Helper function to convert MongoDB data to JSON serializable format
          def mongo_to_json_serializable(data):
              """
              Convert MongoDB data to JSON serializable format by handling ObjectId.
              """
              if isinstance(data, dict):
                  # Handle dictionaries by processing each key-value pair
                  return {k: mongo_to_json_serializable(v) for k, v in data.items()}
              elif isinstance(data, list):
                  # Handle lists by processing each item
                  return [mongo_to_json_serializable(item) for item in data]
              elif isinstance(data, ObjectId):
                  # Convert ObjectId to string
                  return str(data)
              else:
                  # Return other types as is
                  return data

          # Initialize database connections
          def init_connections():
              global redis_client, mongo_client, mongodb, kafka_producer
              
              # Set up Redis connection
              try:
                  redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT)
                  redis_client.ping()  # Test the connection
                  logger.info(f"Connected to Redis at {REDIS_HOST}:{REDIS_PORT}")
              except Exception as e:
                  logger.error(f"Failed to connect to Redis: {e}")
                  redis_client = None
              
              # Set up MongoDB connection
              try:
                  mongo_client = MongoClient(MONGO_URI)
                  mongodb = mongo_client[MONGO_DB]
                  # Create indexes
                  mongodb.sensor_data.create_index([("timestamp", pymongo.DESCENDING)])
                  mongodb.sensor_data.create_index([("sensor_type", pymongo.ASCENDING)])
                  mongodb.sensor_data.create_index([("location_code", pymongo.ASCENDING)])
                  mongodb.sensor_data.create_index([("panel_id", pymongo.ASCENDING)])
                  logger.info(f"Connected to MongoDB at {MONGO_URI}")
              except Exception as e:
                  logger.error(f"Failed to connect to MongoDB: {e}")
                  mongo_client = None
                  mongodb = None

          # Process incoming sensor data
          def process_sensor_data(data):
              """Process and store incoming sensor data."""
              # Ensure timestamp is in the data
              if "timestamp" not in data:
                  data["timestamp"] = datetime.now().isoformat()
              
              # Store in MongoDB for long-term storage
              mongo_data = None
              if mongodb is not None:
                  try:
                      result = mongodb.sensor_data.insert_one(data)
                      # After insertion, data now has an ObjectId
                      if result.inserted_id:
                          # Retrieve the complete document to get the _id
                          mongo_data = mongodb.sensor_data.find_one({"_id": result.inserted_id})
                          logger.info(f"Stored data in MongoDB with ID: {result.inserted_id}")
                  except Exception as e:
                      logger.error(f"Failed to store data in MongoDB: {e}")
              
              # Store latest values in Redis for fast access
              if redis_client is not None:
                  try:
                      # Create a key based on sensor type and ID
                      key_base = f"{data.get('sensor_type', 'unknown')}:{data.get('sensor_id', 'unknown')}"
                      
                      # If we have the MongoDB version, use it (but need to handle ObjectId)
                      redis_data = mongo_data if mongo_data is not None else data
                      
                      # Convert any MongoDB specific types (like ObjectId) to JSON serializable types
                      redis_data = mongo_to_json_serializable(redis_data)
                      
                      # Store the full data JSON
                      redis_client.set(
                          f"{key_base}:latest", 
                          json.dumps(redis_data),
                          ex=86400  # Expire after 24 hours
                      )
                      
                      # Store the value separately if it exists
                      if "value" in data:
                          redis_client.set(
                              f"{key_base}:value", 
                              str(data["value"]),  # Convert to string for safety
                              ex=86400  # Expire after 24 hours
                          )
                      
                      # If it's a thermal image, store defect info
                      if data.get("sensor_type") == "thermal_imaging" and data.get("defect_detected"):
                          # Handle ObjectId in defect info
                          defect_info = mongo_to_json_serializable(data.get("defect_info", {}))
                          redis_client.set(
                              f"{key_base}:defect",
                              json.dumps(defect_info),
                              ex=86400
                          )
                          
                          # Add to a defect list for quick retrieval of all defects
                          defect_data = {
                              "timestamp": data.get("timestamp"),
                              "location_code": data.get("location_code"),
                              "panel_id": data.get("panel_id"),
                              "defect_info": defect_info
                          }
                          redis_client.lpush("defects:recent", json.dumps(defect_data))
                          redis_client.ltrim("defects:recent", 0, 99)  # Keep only the 100 most recent
                      
                      logger.info(f"Stored data in Redis with key: {key_base}:latest")
                  except Exception as e:
                      logger.error(f"Failed to store data in Redis: {e}")

          # API routes
          @app.route('/sensor-data', methods=['POST'])
          def receive_sensor_data():
              """Receive sensor data from pods and process it."""
              if not request.json:
                  return jsonify({"error": "No data provided"}), 400
              
              data = request.json
              logger.info(f"Received data: {data.get('sensor_type')} from {data.get('sensor_id')}")
              
              # Process the data in a separate thread to not block the API
              threading.Thread(target=process_sensor_data, args=(data,)).start()
              
              return jsonify({"status": "received"}), 200

          @app.route('/latest/<sensor_type>', methods=['GET'])
          def get_latest_by_type(sensor_type):
              """Get latest data for a specific sensor type."""
              if redis_client is None:
                  return jsonify({"error": "Redis not available"}), 503
              
              try:
                  # Get all keys matching this sensor type
                  keys = redis_client.keys(f"{sensor_type}:*:latest")
                  
                  results = []
                  for key in keys:
                      data = redis_client.get(key)
                      if data:
                          try:
                              results.append(json.loads(data))
                          except json.JSONDecodeError as e:
                              logger.error(f"Error decoding JSON from Redis: {e}")
                  
                  return jsonify(results), 200
              except Exception as e:
                  logger.error(f"Error retrieving latest data: {e}")
                  return jsonify({"error": str(e)}), 500

          @app.route('/latest/location/<location_code>', methods=['GET'])
          def get_latest_by_location(location_code):
              """Get latest data for a specific location."""
              if mongodb is None:
                  return jsonify({"error": "MongoDB not available"}), 503
              
              try:
                  # Find the latest entry for each sensor type at this location
                  pipeline = [
                      {"$match": {"location_code": location_code}},
                      {"$sort": {"timestamp": -1}},
                      {"$group": {
                          "_id": "$sensor_type",
                          "latest": {"$first": "$$ROOT"}
                      }},
                      {"$replaceRoot": {"newRoot": "$latest"}}
                  ]
                  
                  results = list(mongodb.sensor_data.aggregate(pipeline))
                  
                  # Convert to JSON serializable format
                  results = mongo_to_json_serializable(results)
                  
                  return jsonify(results), 200
              except Exception as e:
                  logger.error(f"Error retrieving location data: {e}")
                  return jsonify({"error": str(e)}), 500

          @app.route('/panel/<panel_id>', methods=['GET'])
          def get_panel_data(panel_id):
              """Get data for a specific panel."""
              if mongodb is None:
                  return jsonify({"error": "MongoDB not available"}), 503
              
              try:
                  # Find the latest entry for each sensor type for this panel
                  pipeline = [
                      {"$match": {"panel_id": panel_id}},
                      {"$sort": {"timestamp": -1}},
                      {"$group": {
                          "_id": "$sensor_type",
                          "latest": {"$first": "$$ROOT"}
                      }},
                      {"$replaceRoot": {"newRoot": "$latest"}}
                  ]
                  
                  results = list(mongodb.sensor_data.aggregate(pipeline))
                  
                  # Convert to JSON serializable format
                  results = mongo_to_json_serializable(results)
                  
                  return jsonify(results), 200
              except Exception as e:
                  logger.error(f"Error retrieving panel data: {e}")
                  return jsonify({"error": str(e)}), 500

          @app.route('/defects/recent', methods=['GET'])
          def get_recent_defects():
              """Get recent defects from all panels."""
              if redis_client is None:
                  return jsonify({"error": "Redis not available"}), 503
              
              try:
                  defects = redis_client.lrange("defects:recent", 0, -1)
                  result = [json.loads(d) for d in defects]
                  return jsonify(result), 200
              except Exception as e:
                  logger.error(f"Error retrieving recent defects: {e}")
                  return jsonify({"error": str(e)}), 500

          @app.route('/history/<sensor_type>/<sensor_id>', methods=['GET'])
          def get_sensor_history(sensor_type, sensor_id):
              """Get historical data for a specific sensor."""
              if mongodb is None:
                  return jsonify({"error": "MongoDB not available"}), 503
              
              try:
                  # Optional query parameters
                  hours = request.args.get('hours', default=24, type=int)
                  limit = request.args.get('limit', default=100, type=int)
                  
                  # Calculate time window
                  end_time = datetime.now()
                  start_time = end_time - timedelta(hours=hours)
                  
                  # Query MongoDB
                  results = list(mongodb.sensor_data.find(
                      {
                          "sensor_type": sensor_type,
                          "sensor_id": sensor_id,
                          "timestamp": {"$gte": start_time.isoformat(), "$lte": end_time.isoformat()}
                      },
                      {"_id": 0}  # Exclude MongoDB _id field
                  ).sort("timestamp", -1).limit(limit))
                  
                  # Convert to JSON serializable format
                  results = mongo_to_json_serializable(results)
                  
                  return jsonify(results), 200
              except Exception as e:
                  logger.error(f"Error retrieving sensor history: {e}")
                  return jsonify({"error": str(e)}), 500

          def cleanup_old_data():
              """Periodically clean up old data from MongoDB."""
              while True:
                  try:
                      if mongodb is not None:
                          # Calculate cutoff date
                          cutoff_date = (datetime.now() - timedelta(days=DATA_RETENTION_DAYS)).isoformat()
                          
                          # Delete old data
                          result = mongodb.sensor_data.delete_many({"timestamp": {"$lt": cutoff_date}})
                          logger.info(f"Deleted {result.deleted_count} old records")
                  except Exception as e:
                      logger.error(f"Error during data cleanup: {e}")
                  
                  # Run once a day
                  time.sleep(86400)

          def main():
              """Main function to start the data collector service."""
              logger.info("Starting Data Collector Service")
              
              # Initialize connections
              init_connections()
              
              # Start cleanup thread
              threading.Thread(target=cleanup_old_data, daemon=True).start()
              
              # Start the API server
              logger.info(f"Starting API server on port {API_PORT}")
              app.run(host='0.0.0.0', port=API_PORT)

          if __name__ == "__main__":
              main()
          EOF
          python /app/collector.py
        env:
        - name: REDIS_HOST
          value: "redis"
        - name: REDIS_PORT
          value: "6379"
        - name: MONGO_URI
          value: "mongodb://mongodb:27017/"
        - name: MONGO_DB
          value: "solar_panel_data"
        - name: USE_KAFKA
          value: "false"
        - name: DATA_RETENTION_DAYS
          value: "30"
        - name: API_PORT
          value: "8080"
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "250m"
